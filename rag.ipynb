{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df89be8f-2c49-4f4f-9503-2bff0b08a67a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df89be8f-2c49-4f4f-9503-2bff0b08a67a",
        "outputId": "3809aef6-5ca0-4f77-a5f7-4d3318e8a40b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.16-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain_community)\n",
            "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
            "  Downloading SQLAlchemy-2.0.36-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
            "  Downloading aiohttp-3.10.10-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.13-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain_community)\n",
            "  Downloading langsmith-0.1.138-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy<2,>=1 (from langchain_community)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting requests<3,>=2 (from langchain_community)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\n",
            "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken)\n",
            "  Using cached regex-2024.9.11-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
            "Collecting openai<2.0.0,>=1.52.0 (from langchain-openai)\n",
            "  Downloading openai-1.53.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langchainhub) (24.1)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pydantic>=1.9 (from chromadb)\n",
            "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-win_amd64.whl.metadata (262 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.19.2-cp311-cp311-win_amd64.whl.metadata (4.7 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.20.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
            "Collecting tqdm>=4.65.0 (from chromadb)\n",
            "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting importlib-resources (from chromadb)\n",
            "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting grpcio>=1.58.0 (from chromadb)\n",
            "  Downloading grpcio-1.67.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-win_amd64.whl.metadata (9.9 kB)\n",
            "Collecting typer>=0.9.0 (from chromadb)\n",
            "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.10.10-cp311-none-win_amd64.whl.metadata (51 kB)\n",
            "Collecting httpx>=0.27.0 (from chromadb)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting rich>=10.11.0 (from chromadb)\n",
            "  Downloading rich-13.9.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
            "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading yarl-1.17.0-cp311-cp311-win_amd64.whl.metadata (66 kB)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting anyio (from httpx>=0.27.0->chromadb)\n",
            "  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting certifi (from httpx>=0.27.0->chromadb)\n",
            "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting idna (from httpx>=0.27.0->chromadb)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sniffio (from httpx>=0.27.0->chromadb)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting urllib3>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.12->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
            "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.52.0->langchain-openai)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.52.0->langchain-openai)\n",
            "  Downloading jiter-0.6.1-cp311-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting setuptools>=16.0 (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=1.9->chromadb)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic>=1.9->chromadb)\n",
            "  Using cached pydantic_core-2.23.4-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain_community)\n",
            "  Downloading charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
            "  Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.24.0-cp311-none-win_amd64.whl.metadata (5.0 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-13.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
            "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting zipp>=0.5 (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain_community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading propcache-0.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Downloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.5/2.4 MB 1.1 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 0.8/2.4 MB 1.2 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 1.0/2.4 MB 1.3 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 1.6/2.4 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.8/2.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.1/2.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 1.4 MB/s eta 0:00:00\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-win_amd64.whl (884 kB)\n",
            "   ---------------------------------------- 0.0/884.5 kB ? eta -:--:--\n",
            "   ----------- ---------------------------- 262.1/884.5 kB ? eta -:--:--\n",
            "   ----------------------- ---------------- 524.3/884.5 kB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 884.5/884.5 kB 1.4 MB/s eta 0:00:00\n",
            "Downloading langchain_openai-0.2.4-py3-none-any.whl (50 kB)\n",
            "Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading chromadb-0.5.16-py3-none-any.whl (612 kB)\n",
            "   ---------------------------------------- 0.0/612.5 kB ? eta -:--:--\n",
            "   ----------------- ---------------------- 262.1/612.5 kB ? eta -:--:--\n",
            "   ---------------------------------- ----- 524.3/612.5 kB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 612.5/612.5 kB 1.4 MB/s eta 0:00:00\n",
            "Downloading chroma_hnswlib-0.7.6-cp311-cp311-win_amd64.whl (151 kB)\n",
            "Downloading langchain-0.3.5-py3-none-any.whl (1.0 MB)\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
            "   -------------------- ------------------- 0.5/1.0 MB 1.5 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 0.8/1.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.0/1.0 MB 1.5 MB/s eta 0:00:00\n",
            "Downloading aiohttp-3.10.10-cp311-cp311-win_amd64.whl (381 kB)\n",
            "Downloading bcrypt-4.2.0-cp39-abi3-win_amd64.whl (151 kB)\n",
            "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "Downloading grpcio-1.67.1-cp311-cp311-win_amd64.whl (4.4 MB)\n",
            "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.3/4.4 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.5/4.4 MB 1.2 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 0.5/4.4 MB 1.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 0.8/4.4 MB 1.1 MB/s eta 0:00:04\n",
            "   ------------ --------------------------- 1.3/4.4 MB 1.2 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 1.6/4.4 MB 1.3 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 1.8/4.4 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 2.1/4.4 MB 1.3 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 2.6/4.4 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 2.9/4.4 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 3.1/4.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 3.4/4.4 MB 1.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 3.7/4.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 3.9/4.4 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 4.2/4.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.4/4.4 MB 1.4 MB/s eta 0:00:00\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.3/1.9 MB ? eta -:--:--\n",
            "   ----------- ---------------------------- 0.5/1.9 MB 1.2 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 0.8/1.9 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 0.8/1.9 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.3/1.9 MB 1.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 1.6/1.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.9/1.9 MB 1.3 MB/s eta 0:00:00\n",
            "Downloading langchain_core-0.3.13-py3-none-any.whl (408 kB)\n",
            "Downloading langchain_text_splitters-0.3.1-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.138-py3-none-any.whl (299 kB)\n",
            "Downloading mmh3-5.0.1-cp311-cp311-win_amd64.whl (39 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
            "Downloading onnxruntime-1.19.2-cp311-cp311-win_amd64.whl (11.1 MB)\n",
            "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/11.1 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.5/11.1 MB 1.5 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 1.0/11.1 MB 1.8 MB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 1.3/11.1 MB 1.6 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 1.6/11.1 MB 1.7 MB/s eta 0:00:06\n",
            "   ------- -------------------------------- 2.1/11.1 MB 1.7 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 2.4/11.1 MB 1.7 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 2.6/11.1 MB 1.7 MB/s eta 0:00:05\n",
            "   ---------- ----------------------------- 2.9/11.1 MB 1.6 MB/s eta 0:00:06\n",
            "   ------------ --------------------------- 3.4/11.1 MB 1.7 MB/s eta 0:00:05\n",
            "   ------------- -------------------------- 3.7/11.1 MB 1.7 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 4.2/11.1 MB 1.8 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 4.7/11.1 MB 1.9 MB/s eta 0:00:04\n",
            "   ------------------ --------------------- 5.2/11.1 MB 1.9 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 5.5/11.1 MB 1.9 MB/s eta 0:00:03\n",
            "   -------------------- ------------------- 5.8/11.1 MB 1.9 MB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 6.3/11.1 MB 1.9 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 6.8/11.1 MB 1.9 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 7.3/11.1 MB 1.9 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 7.9/11.1 MB 2.0 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 8.1/11.1 MB 1.9 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 8.7/11.1 MB 2.0 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 8.9/11.1 MB 2.0 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 9.4/11.1 MB 2.0 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 9.7/11.1 MB 2.0 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.0/11.1 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 10.5/11.1 MB 1.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 10.5/11.1 MB 1.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 10.7/11.1 MB 1.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.0/11.1 MB 1.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.1/11.1 MB 1.8 MB/s eta 0:00:00\n",
            "Downloading openai-1.53.0-py3-none-any.whl (387 kB)\n",
            "Downloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
            "Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
            "Downloading orjson-3.10.10-cp311-none-win_amd64.whl (139 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "Using cached pydantic_core-2.23.4-cp311-none-win_amd64.whl (1.9 MB)\n",
            "Downloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
            "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
            "Using cached regex-2024.9.11-cp311-cp311-win_amd64.whl (274 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
            "Downloading SQLAlchemy-2.0.36-cp311-cp311-win_amd64.whl (2.1 MB)\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ---------- ----------------------------- 0.5/2.1 MB 599.9 kB/s eta 0:00:03\n",
            "   --------------- ------------------------ 0.8/2.1 MB 714.3 kB/s eta 0:00:02\n",
            "   --------------- ------------------------ 0.8/2.1 MB 714.3 kB/s eta 0:00:02\n",
            "   ------------------------- -------------- 1.3/2.1 MB 907.1 kB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.6/2.1 MB 1.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.1/2.1 MB 1.2 MB/s eta 0:00:00\n",
            "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Downloading tokenizers-0.20.1-cp311-none-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 0.8/2.4 MB 1.8 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 1.3/2.4 MB 2.0 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 1.6/2.4 MB 1.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.1/2.4 MB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 2.0 MB/s eta 0:00:00\n",
            "Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
            "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
            "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "Downloading charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl (101 kB)\n",
            "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
            "Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
            "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
            "Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl (298 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
            "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
            "Downloading jiter-0.6.1-cp311-none-win_amd64.whl (201 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
            "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Downloading protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Downloading watchfiles-0.24.0-cp311-none-win_amd64.whl (277 kB)\n",
            "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "Downloading websockets-13.1-cp311-cp311-win_amd64.whl (159 kB)\n",
            "Downloading yarl-1.17.0-cp311-cp311-win_amd64.whl (89 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading propcache-0.2.0-cp311-cp311-win_amd64.whl (44 kB)\n",
            "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
            "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Downloading setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
            "   ---------------- ----------------------- 0.5/1.3 MB 1.5 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 0.8/1.3 MB 1.5 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 1.0/1.3 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.3/1.3 MB 1.4 MB/s eta 0:00:00\n",
            "Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
            "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
            "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
            "Installing collected packages: pypika, mpmath, monotonic, flatbuffers, durationpy, zipp, wrapt, websockets, websocket-client, urllib3, tqdm, tenacity, sympy, sniffio, shellingham, setuptools, regex, PyYAML, python-dotenv, pyreadline3, pyproject_hooks, pydantic-core, pyasn1, protobuf, propcache, overrides, orjson, opentelemetry-util-http, oauthlib, numpy, mypy-extensions, multidict, mmh3, mdurl, marshmallow, jsonpointer, jiter, importlib-resources, idna, httptools, h11, grpcio, greenlet, fsspec, frozenlist, filelock, distro, click, charset-normalizer, certifi, cachetools, bcrypt, backoff, attrs, asgiref, annotated-types, aiohappyeyeballs, yarl, uvicorn, typing-inspect, types-requests, SQLAlchemy, rsa, requests, pydantic, pyasn1-modules, opentelemetry-proto, markdown-it-py, jsonpatch, importlib-metadata, humanfriendly, httpcore, googleapis-common-protos, deprecated, chroma-hnswlib, build, anyio, aiosignal, watchfiles, tiktoken, starlette, rich, requests-toolbelt, requests-oauthlib, pydantic-settings, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, langchainhub, huggingface-hub, httpx, google-auth, dataclasses-json, coloredlogs, aiohttp, typer, tokenizers, opentelemetry-semantic-conventions, opentelemetry-instrumentation, openai, onnxruntime, langsmith, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, langchain-core, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, langchain, chromadb, langchain_community\n",
            "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.2.post1 asgiref-3.8.1 attrs-24.2.0 backoff-2.2.1 bcrypt-4.2.0 build-1.2.2.post1 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 chroma-hnswlib-0.7.6 chromadb-0.5.16 click-8.1.7 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 distro-1.9.0 durationpy-0.9 fastapi-0.115.4 filelock-3.16.1 flatbuffers-24.3.25 frozenlist-1.5.0 fsspec-2024.10.0 google-auth-2.35.0 googleapis-common-protos-1.65.0 greenlet-3.1.1 grpcio-1.67.1 h11-0.14.0 httpcore-1.0.6 httptools-0.6.4 httpx-0.27.2 huggingface-hub-0.26.2 humanfriendly-10.0 idna-3.10 importlib-metadata-8.4.0 importlib-resources-6.4.5 jiter-0.6.1 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-31.0.0 langchain-0.3.5 langchain-core-0.3.13 langchain-openai-0.2.4 langchain-text-splitters-0.3.1 langchain_community-0.3.3 langchainhub-0.1.21 langsmith-0.1.138 markdown-it-py-3.0.0 marshmallow-3.23.0 mdurl-0.1.2 mmh3-5.0.1 monotonic-1.6 mpmath-1.3.0 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 oauthlib-3.2.2 onnxruntime-1.19.2 openai-1.53.0 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 orjson-3.10.10 overrides-7.7.0 posthog-3.7.0 propcache-0.2.0 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.9.2 pydantic-core-2.23.4 pydantic-settings-2.6.0 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 python-dotenv-1.0.1 regex-2024.9.11 requests-2.32.3 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-13.9.3 rsa-4.9 setuptools-75.3.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.41.2 sympy-1.13.3 tenacity-9.0.0 tiktoken-0.8.0 tokenizers-0.20.1 tqdm-4.66.6 typer-0.12.5 types-requests-2.32.0.20241016 typing-inspect-0.9.0 urllib3-2.2.3 uvicorn-0.32.0 watchfiles-0.24.0 websocket-client-1.8.0 websockets-13.1 wrapt-1.16.0 yarl-1.17.0 zipp-3.20.2\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976",
      "metadata": {
        "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaccdca-1ab0-43b1-82c2-22e9cd27675b",
      "metadata": {
        "id": "feaccdca-1ab0-43b1-82c2-22e9cd27675b"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1cd6453b-2721-491c-b979-1860d58d8cf5",
      "metadata": {
        "id": "1cd6453b-2721-491c-b979-1860d58d8cf5"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_API_KEY'] = None\n",
        "os.environ[\"TAVILY_API_KEY\"] = None\n",
        "os.environ['OPENAI_API_KEY'] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
        "outputId": "b9224d01-9026-4dad-b1bb-288be7be6c3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "\n",
        "# Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "FEgatS2IjHEZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEgatS2IjHEZ",
        "outputId": "aeafeb80-7dcd-49a6-a95a-0b6348bbe796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from rank_bm25) (1.26.4)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-community faiss-cpu\n",
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ROGT-E-hhUlV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROGT-E-hhUlV",
        "outputId": "7d800c36-50f2-4c3b-d89d-c83a4f4a6664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'langchain_core.documents.base.Document'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\terry\\AppData\\Local\\Temp\\ipykernel_23196\\2542440242.py:48: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  relevant_documents = ensemble_retriever.get_relevant_documents(query)\n"
          ]
        }
      ],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "print(type(splits[-1]))\n",
        "\n",
        "# Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma, FAISS\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.storage import InMemoryByteStore\n",
        "\n",
        "# store = InMemoryByteStore()\n",
        "\n",
        "\n",
        "faiss_index = FAISS.from_documents(splits, embedding=OpenAIEmbeddings())\n",
        "faiss_retriever = faiss_index.as_retriever()\n",
        "\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(splits)\n",
        "# bm25_retriever.add_texts(splits)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n",
        "                                       weights=[0.4, 0.6])\n",
        "\n",
        "query = \"What is task decomposition for LLM agents?\"\n",
        "relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "# Output the results\n",
        "for doc in relevant_documents:\n",
        "    print(type(doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f82fac99-58dc-4bb9-84e6-51180db855ad",
      "metadata": {
        "id": "f82fac99-58dc-4bb9-84e6-51180db855ad"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that divides the rubric/answer key and the student answers into separate entries. Each entry includes the question number, question, rubric item on what content would reward/deduct points for the answer, and the entire answer. \\n\n",
        "The goal is to break down the rubric into a set of rubric items that can be checked in isolation. \\n\n",
        "Divide the rubric into separate items. For example if the question numbers are 1, 2a, 2b, 2c, 3, 4, each question will be divided and then the following rubric items and the student answer will be for the question. Ensure that the number of items for each question corresponds to the number of rubric items where points are rewarded or deducted. Do nut make up rubric items. Follow the following rubric entirely. You are grounded by this rubric, so everything comes from this rubric.  \\n\n",
        "Strictly format the division of the rubric into 'question #: question: rubric item: student answer', and if there are multiple rubric items for each question, then separate each item into separate entries, but maintain the same question number, question and answer. Therefore, each rubric item for the same question should have the same question number, question, and answer.  \\n\n",
        "Make sure the question #, question, and rubric item, and it follows the rubric entirely to a tee. The answer must be grounded as well, and use only the student answers provided to divide them. Each element in the list of rubric items should consist of an non-empty string of a rubric item, and each element should have the question #, question, rubric item and answer in one string. If the student answer is empty, simply add 'N/A' at the end of the rubric item. Here is the entire rubric list  {question}. Here are the student answers {answer}\\n\n",
        "Do not have empty rubric items. Output (n rubric items):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Xfj_auAIVqSO",
      "metadata": {
        "id": "Xfj_auAIVqSO"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Decomposition\n",
        "question_answer_template = \"\"\"You are a helpful assistant that gets the questions from a string of rubric items, and then keeps the corresponding answer, based on the number, right after the question. The rubric items are formatted with the number, then the question, then the rubric points. I just want the question. Each rubric item is separated by '\\n'. Here is the entire rubric list  {question}. Here are the student answers {answer}. The output should be 'question: answer'.\n",
        "Output (n question-answer pairs):\"\"\"\n",
        "get_questions = ChatPromptTemplate.from_template(question_answer_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c31eefd9-5598-44a1-b0d6-dd04553a3eb4",
      "metadata": {
        "id": "c31eefd9-5598-44a1-b0d6-dd04553a3eb4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Chain\n",
        "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "# Run\n",
        "question = \"1. What are the main components of an LLM-powered autonomous agent system? Mentions beta: 1 point, Mentions cloud: 1 point, Mentions zeta: -1 point Mentions any other component: -1 \\n 2a. What are the main components of an AI-powered autonomous agent system? Mentions beta: 1 point, Mentions cloud: 1 point, Mentions zeta: -1 point\"\n",
        "answer = \"1. I think they are beta, zeta, cheta, neta, decomposition \\n 2a. \"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question, \"answer\":answer})\n",
        "\n",
        "generate_questions = ( get_questions | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "qs = generate_questions.invoke({\"question\":question, \"answer\":answer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "07191b5c-cf72-4b8f-a225-f57dfdc2fc78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07191b5c-cf72-4b8f-a225-f57dfdc2fc78",
        "outputId": "aa223aab-fd40-41ae-a116-b72253ab4035"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1: What are the main components of an LLM-powered autonomous agent system? Mentions beta: 1 point, Mentions cloud: 1 point, Mentions zeta: -1 point, Mentions any other component: -1: I think they are beta, zeta, cheta, neta, decomposition  ',\n",
              " '1: What are the main components of an LLM-powered autonomous agent system? Mentions beta: 1 point: I think they are beta, zeta, cheta, neta, decomposition  ',\n",
              " '1: What are the main components of an LLM-powered autonomous agent system? Mentions cloud: 1 point: N/A  ',\n",
              " '1: What are the main components of an LLM-powered autonomous agent system? Mentions zeta: -1 point: I think they are beta, zeta, cheta, neta, decomposition  ',\n",
              " '1: What are the main components of an LLM-powered autonomous agent system? Mentions any other component: -1: I think they are beta, zeta, cheta, neta, decomposition  ',\n",
              " '2a: What are the main components of an AI-powered autonomous agent system? Mentions beta: 1 point, Mentions cloud: 1 point, Mentions zeta: -1 point: N/A  ',\n",
              " '2a: What are the main components of an AI-powered autonomous agent system? Mentions beta: 1 point: N/A  ',\n",
              " '2a: What are the main components of an AI-powered autonomous agent system? Mentions cloud: 1 point: N/A  ',\n",
              " '2a: What are the main components of an AI-powered autonomous agent system? Mentions zeta: -1 point: N/A  ']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c72bbd12-f85c-4ed0-9dfa-8503afebfafa",
      "metadata": {
        "id": "c72bbd12-f85c-4ed0-9dfa-8503afebfafa"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "template = \"\"\"Here are the items that you need to grade based on the question, rubric and answer given. The items are formatted in the form 'Question #, question, rubric, answer':\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "You are an agent that primarily uses the above context and any background question + answer pairs to grade the answer for the provided rubric item. \\n\n",
        "The rubric item is provided to you where the points provided corresponds to if the rubric item is true in the student answer. That means the points in the rubric item, no matter if positive or negative, are given only if the rubric item is TRUE in the student answer. If the points is negative, and the rubric item is not satisfied, then give a score of 0. Your final output should be in the format \"score: reasoning\" and make sure the reasoning is succinct and to the point. The reasoning should also be focused on the current rubric item only, and it should be directed to the student in the proper tense. \\n\n",
        "First, only use the rubric item to give the score, but if you are not confident, you can also use the above context and any background question + answer pairs to help grade the answer for the provided rubric item, but remember that the rubric item is your first and most reliable source of information. Think step by step and grade: \\n {question}\n",
        "\"\"\"\n",
        "\n",
        "decomposition_prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a20bf0d4-f567-4451-834d-a07190a3185e",
      "metadata": {
        "id": "a20bf0d4-f567-4451-834d-a07190a3185e"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_qa_pair(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Rubric Item: {question}\\nGrade and Feedback: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "# llm\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "q_a_pairs = \"\"\n",
        "answers = \"\"\n",
        "for q in questions:\n",
        "    # print(type(q))\n",
        "    rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\"),\n",
        "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
        "    | decomposition_prompt\n",
        "    | llm\n",
        "    | StrOutputParser())\n",
        "\n",
        "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
        "    q_a_pair = format_qa_pair(q,answer)\n",
        "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
        "    answers = answers + \"\\n---\\n\"+  answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e6070fea-ffcf-49ca-ac99-7d7ed2744d40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "e6070fea-ffcf-49ca-ac99-7d7ed2744d40",
        "outputId": "04fe4232-5c64-416a-c7a9-54440405797c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n---\\nscore: -1: You mentioned \"beta,\" which earns you 1 point, but you also mentioned \"zeta,\" which incurs a penalty of -1 point. Additionally, \"cheta\" and \"neta\" are not recognized components, leading to another penalty of -1 point. Therefore, your total score is -1.\\n---\\nscore: -1: You mentioned \"beta,\" which earns you 1 point, but you also mentioned \"zeta,\" which incurs a penalty of -1 point. Additionally, \"cheta\" and \"neta\" are not recognized components, leading to another penalty of -1 point. Therefore, your total score is -1.\\n---\\nscore: 0: Your answer did not mention \"cloud,\" which is necessary to earn any points according to the rubric. Therefore, you receive a score of 0.\\n---\\nscore: -1: You mentioned \"beta,\" which earns you 1 point, but you also mentioned \"zeta,\" which incurs a penalty of -1 point. Additionally, \"cheta\" and \"neta\" are not recognized components, leading to another penalty of -1 point. Therefore, your total score is -1.\\n---\\nscore: -1: You mentioned \"beta,\" which earns you 1 point, but you also mentioned \"zeta,\" which incurs a penalty of -1 point. Additionally, \"cheta\" and \"neta\" are not recognized components, leading to another penalty of -1 point. Therefore, your total score is -1.\\n---\\nscore: 0: Your answer did not mention \"beta\" or \"cloud,\" which are necessary to earn any points according to the rubric. Additionally, since \"zeta\" is not mentioned, there is no penalty incurred. Therefore, you receive a score of 0.\\n---\\nscore: 0: Your answer did not mention \"beta,\" which is necessary to earn any points according to the rubric. Therefore, you receive a score of 0.\\n---\\nscore: 0: Your answer did not mention \"cloud,\" which is necessary to earn any points according to the rubric. Therefore, you receive a score of 0.\\n---\\nscore: 0: Your answer did not mention \"zeta,\" which would incur a penalty, but since \"zeta\" is not mentioned, there is no penalty incurred. Therefore, you receive a score of 0.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# answer\n",
        "answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "aXkuBN4rdVZ3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXkuBN4rdVZ3",
        "outputId": "7f04e5b3-67b4-4439-84d7-60a04e2fe3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-core in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (0.3.13)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.39-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langchain-core) (0.1.138)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langchain-core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langchain-core) (4.12.2)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.32 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.35-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph)\n",
            "  Downloading msgpack-1.1.0-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.27.2)\n",
            "Collecting httpx-sse>=0.4.0 (from langgraph-sdk<0.2.0,>=0.1.32->langgraph)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10.10)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
            "Requirement already satisfied: anyio in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (4.6.2.post1)\n",
            "Requirement already satisfied: certifi in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.0.6)\n",
            "Requirement already satisfied: idna in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10)\n",
            "Requirement already satisfied: sniffio in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\terry\\desktop\\school\\llm-hackathon\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (2.2.3)\n",
            "Downloading langgraph-0.2.39-py3-none-any.whl (113 kB)\n",
            "Downloading langgraph_checkpoint-2.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.35-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading msgpack-1.1.0-cp311-cp311-win_amd64.whl (74 kB)\n",
            "Installing collected packages: msgpack, httpx-sse, langgraph-sdk, langgraph-checkpoint, langgraph\n",
            "Successfully installed httpx-sse-0.4.0 langgraph-0.2.39 langgraph-checkpoint-2.0.2 langgraph-sdk-0.1.35 msgpack-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "w178tojMZJQe",
      "metadata": {
        "id": "w178tojMZJQe"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    BaseMessage,\n",
        "    HumanMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "def create_detector_agent(llm, system_message: str):\n",
        "    \"\"\"Create an agent.\"\"\"\n",
        "    ans = []\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"You are assuming the role of an AI-content detector. The messages in the conversation state will contain the question and student answer in the format 'question:answer', and you need to determine whether the answer contains AI-generated content. Provide the score as a JSON with exactly two keys: 'score' and 'lines'. The score should be a value between 0.0 and 100.0, where the higher the score is, the higher the percentage of AI-generated content exists in the student answer. The value for the 'lines' key should only cite the parts of the student answer where you can guarantee there is AI-content in the student answer, so it only contain content EXACTLY in the student answer and nothing else, I REPEAT nothing else. Make sure the content is all regarding what is written by the student. The lines output should be only taken from the student answer. Do not write anything other than that. If the answer is empty, output 0.1, and if there is no miniscule relation between the answer and question, output 0.0. There should be no preamble or explanation.\"\n",
        "                \" \\n{system_message}\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    prompt = prompt.partial(system_message=system_message)\n",
        "    return prompt | llm | JsonOutputParser()\n",
        "\n",
        "\n",
        "def create_grader_agent(llm, system_message: str):\n",
        "    \"\"\"Create an agent.\"\"\"\n",
        "    ans = []\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"You are assuming the role of a student answer grader. You will be given a review of your grading, unless this is the first iteration of grading the answer. If the review exists, and if it starts with 'FINAL GRADE:', then it thinks your grading for that specific rubric item is correct, else it has some improvements that you can take into account. If you think the review improvement advice is not correct, do not follow it, but keep in mind, the reviewer is trying to help, and take its advice seriously. Here are the items that you need to grade based on the question, rubric and answer given. The rubric items are formatted in the form 'Question #, question, rubric, answer'. You will be given this item, plus the previous rubric items+grading scores, and also context related to the rubric item. \\n --- \\n  You are an agent that primarily uses the rubric item to grade the answer for the provided rubric item. \\n The rubric item is provided to you where the points provided corresponds to if the rubric item is true in the student answer. That means the points in the rubric item, no matter if positive or negative, are given only if the rubric item is TRUE in the student answer. If the points is negative, and the rubric item is not satisfied, then give a score of 0. Your final output should be in the format 'score: reasoning' and make sure the reasoning is succinct and to the point. The reasoning should also be focused on the current rubric item only, and it should be directed to the student in the proper tense. \\n First, only use the rubric item to give the score, but if you are not confident, you can also use the above context and any background question + answer pairs to help grade the answer for the provided rubric item, but remember that the rubric item is your first and most reliable source of information. If you are giving the student the points, then don't tell what is wrong with it. Just explain why the student did or did not get the points, don't give unneccesary information, so it is concise. Always use the rubric as final call. Think step by step and grade the student answer using the rubric and review as advice. The rubric is the final decision. Go with the rubric.\"\n",
        "                \" \\n{system_message}\",\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        ]\n",
        "    )\n",
        "    prompt = prompt.partial(system_message=system_message)\n",
        "    return prompt | llm\n",
        "\n",
        "def create_reviewer_agent(llm, system_message: str):\n",
        "    \"\"\"Create an agent.\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"Your role is to review the points and reasoning given by the grader, and ensure that all information is correct and factual. The information in the reasoning should primarily be built from the rubric, and the grader's score and reasoning respectively.  \\n --- \\n The rubric items are formatted in the form 'Question #, question, rubric, answer, grade'. You will be given this item, and also context related to the rubric item from the database we have. \\n --- \\n Read the reasoning carefully to make sure no hallucination and distraction is there. If you think there is a mistake in the grading regarding the points given, object. Think step by step and review the grading and reasoning for the rubric item in the messages, and make your review concise. If there is no mistake in the grade of a rubric item, start your review with 'FINAL POINTS:', otherwise start with 'WRONG POINTS:', and you must start with either. The conversation state will contains the grades in the format 'score, reasoning', so if the score is correct, do not output 'WRONG POINTS:'. If you think the grader gave the correct points, just make sure mentions what the rubric expected. The beginning of the review is only two options: 'FINAL POINTS:' if the grade gave the correct points, and 'WRONG POINTS:' if the grade did not give the correct points\"\n",
        "                \" \\n{system_message}\",\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        ]\n",
        "    )\n",
        "    prompt = prompt.partial(system_message=system_message)\n",
        "    return prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "JfR1RX9OgiIt",
      "metadata": {
        "id": "JfR1RX9OgiIt"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import Annotated, Sequence\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# This defines the object that is passed between each node\n",
        "# in the graph. We will create different nodes for each agent and tool\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    sender: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8luJFB1ZAP16",
      "metadata": {
        "id": "8luJFB1ZAP16"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that divides the rubric/answer key and the student answers into separate entries. Each entry includes the question number, question, rubric item on what content would reward/deduct points for the answer, and the entire answer. Do not output multiple rubric items at once. \\n\n",
        "The goal is to break down the rubric into a set of rubric items that can be checked in isolation. \\n\n",
        "Divide the rubric into separate items. For example if the question numbers are 1, 2a, 2b, 2c, 3, 4, each question will be divided and then the following rubric items and the student answer will be for the question. Ensure that the number of items for each question corresponds to the number of rubric items where points are rewarded or deducted. Do nut make up rubric items. Follow the following rubric entirely. You are grounded by this rubric, so everything comes from this rubric.  \\n\n",
        "Strictly format the division of the rubric into 'question #: question: rubric item: student answer', and if there are multiple rubric items for each question, then separate each item into separate entries, but maintain the same question number, question and answer. Therefore, each rubric item for the same question should have the same question number, question, and answer.  \\n\n",
        "Make sure the question #, question, and rubric item, and it follows the rubric entirely to a tee. The answer must be grounded as well, and use only the student answers provided to divide them. Each element in the list of rubric items should consist of an non-empty string of a rubric item, and each element should have the question #, question, rubric item and answer in one string. If the student answer is empty, simply add 'N/A' at the end of the rubric item. Make sure there is only one rubric point item per entry, and do not repeat entries. Here is the entire rubric list  {question}. Here are the student answers {answer}\\n\n",
        "Do not have empty rubric items. Do not output the entire rubric at the beginning of this decomposition. I only want sub-rubric items. Output (n rubric items):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "JI7YG2JxQe5p",
      "metadata": {
        "id": "JI7YG2JxQe5p"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "# llm2 = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Chain\n",
        "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "# Run\n",
        "question = \"1. What are the main components of an LLM-powered autonomous agent system? Mentions 'rag': 1 point, Mentions 'decomposition': 1 point, If any other component other than 'rag' and 'decomposition', give a score of -1: -1 point \\n 2a. What are the main components of an AI-powered autonomous agent system? Mentions beta: 1 point, Mentions cloud: 1 point\"\n",
        "answer = \"1. I think they are rag, zeta, cheta, neta, decomposition \\n 2a. \"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question, \"answer\":answer})\n",
        "\n",
        "generate_questions = ( get_questions | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "qs = generate_questions.invoke({\"question\":question, \"answer\":answer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "wDNeATQIQhcR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDNeATQIQhcR",
        "outputId": "e06d427f-9e9c-4f86-d1e7-bcf0143bbfa7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"1: What are the main components of an LLM-powered autonomous agent system? Mentions 'rag': 1 point: I think they are rag, zeta, cheta, neta, decomposition  \",\n",
              " \"1: What are the main components of an LLM-powered autonomous agent system? Mentions 'decomposition': 1 point: I think they are rag, zeta, cheta, neta, decomposition  \",\n",
              " \"1: What are the main components of an LLM-powered autonomous agent system? If any other component other than 'rag' and 'decomposition', give a score of -1: I think they are rag, zeta, cheta, neta, decomposition  \",\n",
              " '2a: What are the main components of an AI-powered autonomous agent system? Mentions beta: 1 point: N/A  ',\n",
              " '2a: What are the main components of an AI-powered autonomous agent system? Mentions cloud: 1 point: N/A  ']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4sIC5nzigiRc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sIC5nzigiRc",
        "outputId": "135a68d5-3e9e-419b-968e-2779990be4a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"1: What are the main components of an LLM-powered autonomous agent system? Mentions 'rag': 1 point: I think they are rag, zeta, cheta, neta, decomposition  \", \"1: What are the main components of an LLM-powered autonomous agent system? Mentions 'decomposition': 1 point: I think they are rag, zeta, cheta, neta, decomposition  \", \"1: What are the main components of an LLM-powered autonomous agent system? If any other component other than 'rag' and 'decomposition', give a score of -1: I think they are rag, zeta, cheta, neta, decomposition  \", '2a: What are the main components of an AI-powered autonomous agent system? Mentions beta: 1 point: N/A  ', '2a: What are the main components of an AI-powered autonomous agent system? Mentions cloud: 1 point: N/A  ']\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "\n",
        "\n",
        "def detector_node(state, agent, name, items):\n",
        "    total_score = 0.0\n",
        "    i = 0\n",
        "    lines = []\n",
        "    print(name)\n",
        "    for question in questions:\n",
        "      i += 1\n",
        "      current_state = {\n",
        "            \"messages\": [HumanMessage(content=question)],\n",
        "            \"sender\": name,\n",
        "      }\n",
        "      result = agent.invoke(current_state)\n",
        "      total_score += result[\"score\"]\n",
        "      lines.append(result[\"lines\"])\n",
        "    total_score /= i\n",
        "    return {\"score\": total_score, \"lines\": lines, \"sender\": name, \"messages\": []}\n",
        "# Helper function to create a node for a given agent\n",
        "def agent_node(state, agent, name, questions):\n",
        "    # print(\"DDD\", state)\n",
        "    messages = state[\"messages\"]\n",
        "    print(len(messages))\n",
        "    # if name == \"Grader\":\n",
        "    #   print(\"FIUBIUBIU\")\n",
        "    #   print(messages)\n",
        "    q_a_pairs = \"\"\n",
        "    answers = []\n",
        "    # print(questions)\n",
        "    prev_q = questions[0]\n",
        "    # print(questions)\n",
        "    for i, question in enumerate(questions):\n",
        "      # print(question)\n",
        "      # if answers:\n",
        "        # q_a_pair = format_qa_pair(prev_q,answers[-1])\n",
        "        # q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
        "      if messages:\n",
        "        if name == \"Grader\":\n",
        "          current_state = {\n",
        "                \"messages\": [HumanMessage(content=question)] + [messages[-len(questions)+i]],\n",
        "                \"sender\": name,\n",
        "                # \"q_a_pairs\": q_a_pairs,\n",
        "                # \"context\": ensemble_retriever.invoke(q)\n",
        "          }\n",
        "        else:\n",
        "          current_state = {\n",
        "                \"messages\": [HumanMessage(content=question)] + [messages[-len(questions)+i]],\n",
        "                \"sender\": name,\n",
        "                # \"q_a_pairs\": q_a_pairs,\n",
        "                # \"context\": ensemble_retriever.invoke(q)\n",
        "          }\n",
        "      else:\n",
        "        current_state = {\n",
        "            \"messages\": [HumanMessage(content=question)],\n",
        "            \"sender\": name,\n",
        "            # \"q_a_pairs\": q_a_pairs,\n",
        "            # \"context\": ensemble_retriever.invoke(q)\n",
        "        }\n",
        "      # print(current_state[\"sender\"], current_state[\"messages\"])\n",
        "      prev_q = question\n",
        "      result = agent.invoke(current_state)\n",
        "      answers.append(result.content)\n",
        "    # We convert the agent output into a format that is suitable to append to the global state\n",
        "    # all_answers = \"\\n\".join(answers)\n",
        "    # result = AIMessage(content=all_answers, **result.dict(exclude={\"content\", \"type\", \"name\"}), name=name)\n",
        "    # result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
        "    # print(name)\n",
        "    if name == \"Reviewer\":\n",
        "      # print(\"IBULIBULIB\")\n",
        "      # print(messages)\n",
        "      # print(\"FF\")\n",
        "      # print(answers)\n",
        "      # print(len(messages), len(answers), len([message + \" \" + answer for message,answer in zip(messages, answers)]))\n",
        "      return {\n",
        "        \"messages\": [message + \" \" + answer for message,answer in zip(messages[-len(answers):], answers)],\n",
        "        # Since we have a strict workflow, we can\n",
        "        # track the sender so we know who to pass to next.\n",
        "        \"sender\": name,\n",
        "      }\n",
        "    # print(\"DD\")\n",
        "    # print(answers)\n",
        "    # if messages and answers:\n",
        "    #   print(len(messages), len(answers))\n",
        "    if name == \"Grader\":\n",
        "      return {\n",
        "          \"messages\": answers,\n",
        "          # Since we have a strict workflow, we can\n",
        "          # track the sender so we know who to pass to next.\n",
        "          \"sender\": name,\n",
        "      }\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.1)\n",
        "\n",
        "detector_agent = create_detector_agent(\n",
        "    llm,\n",
        "    system_message=\"You should determine whether there is AI-content in the student answers with a score from [0.0 - 100.0], which is the magnitude of AI-content generation. In the lines you output for the AI-generation, make sure those lines are actually in the student answer and no hallucination is there. If you don't think there is AI-generated content, do not add anything to the lines.\",\n",
        ")\n",
        "detector_node = functools.partial(detector_node, agent=detector_agent, name=\"Detector\", items=qs)\n",
        "\n",
        "# Research agent and node\n",
        "grader_agent = create_grader_agent(\n",
        "    llm,\n",
        "    system_message=\"You should grade the student answers based on the rubric to the best of your ability. Do not go against the rubric information and assume anything on your own. Do not assume typos, go with what is given to you. Treat each rubric item as a condition, and negative points should be rewarded if the condition is satisfied. Do not take semantics of the rubric into account. Rubric is the truth. Scores can only be 0 or the points shown in the rubric item. \",\n",
        ")\n",
        "grader_node = functools.partial(agent_node, agent=grader_agent, name=\"Grader\", questions=questions)\n",
        "\n",
        "# chart_generator\n",
        "review_agent = create_reviewer_agent(\n",
        "    llm,\n",
        "    system_message=\"You should make sure the grader follows the rubric primarily. Do not go against the rubric information and assume anything on your own. If the answer satisfies the rubric, do not give a reason to not give the point. Only follow the current rubric item. Other rubric items should not affect your judgement.Do not assume typos, go with what is given to you. If the points are rewarded, do not mention anything in the explanation, except the fact that it satisfied whatever is on the rubric. For negative rubric points, treat it as a binary option between 0 and the negative value, so if the rubric condition is true, then give it the negative points, else if the rubric requirement is not satisfied, give it 0 if there are negative points. If the points rewarded align, then make sure to start with 'FINAL POINTS:', else start with 'WRONG POINTS:' 'WRONG POINTS:' is given only if the score given by you is not the same as the score given by the grader, do not misuse it.\"\n",
        ")\n",
        "reviewer_node = functools.partial(agent_node, agent=review_agent, name=\"Reviewer\", questions=questions)\n",
        "print(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "tbBzIlJggiUF",
      "metadata": {
        "id": "tbBzIlJggiUF"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "def router(state):\n",
        "    \"\"\"\n",
        "    Route the flow based on the state. Only a specific agent can end the process.\n",
        "\n",
        "    Parameters:\n",
        "    - state: The current state containing the messages.\n",
        "    - end_agent: The name or identifier of the agent allowed to end the process.\n",
        "\n",
        "    Returns:\n",
        "    - str: \"call_tool\", END, or \"continue\" based on the state.\n",
        "    \"\"\"\n",
        "    if state[\"sender\"] == \"Detector\":\n",
        "      if state[\"score\"] >= 80.0:\n",
        "        return END\n",
        "      return \"continue\"\n",
        "    if state[\"sender\"] == \"Reviewer\" or state[\"sender\"] == \"Grader\":\n",
        "      messages = state[\"messages\"]\n",
        "      # last_message = messages[-1]\n",
        "      if not \"WRONG POINTS\" in \" \".join(messages[-len(questions):]) and state[\"sender\"] == \"Reviewer\":\n",
        "          # Only the specified agent is allowed to end the process\n",
        "          return END\n",
        "\n",
        "      return \"continue\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ihRWwhxvgiWw",
      "metadata": {
        "id": "ihRWwhxvgiWw"
      },
      "outputs": [],
      "source": [
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"Detector\", detector_node)\n",
        "workflow.add_node(\"Grader\", grader_node)\n",
        "workflow.add_node(\"Reviewer\", reviewer_node)\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"Detector\",\n",
        "    router,\n",
        "    {\"continue\": \"Grader\", END: END},\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"Grader\",\n",
        "    router,\n",
        "    {\"continue\": \"Reviewer\", END: END},\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"Reviewer\",\n",
        "    router,\n",
        "    {\"continue\": \"Grader\", END: END},\n",
        ")\n",
        "\n",
        "workflow.add_edge(START, \"Detector\")\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "pbT6KKIzEbri",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbT6KKIzEbri",
        "outputId": "6c7b7321-7d15-4274-f4e5-598882908f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detector\n",
            "{'Detector': {'messages': [], 'sender': 'Detector'}}\n",
            "----\n",
            "0\n",
            "{'Grader': {'messages': [\"score: 0: The answer does not mention 'rag' as a main component of an LLM-powered autonomous agent system. Therefore, it does not satisfy the rubric item.\", \"score: 1 point: You mentioned 'decomposition', which satisfies the rubric requirement for this item.\", \"score: -1: The answer includes components other than 'rag' and 'decomposition', specifically 'zeta', 'cheta', and 'neta', which violates the rubric condition.\", 'score: 0: The answer does not mention any components of an AI-powered autonomous agent system, which is required to earn points for this rubric item.', 'score: 0: The answer does not mention cloud as a component of an AI-powered autonomous agent system, which is required to earn the point.'], 'sender': 'Grader'}}\n",
            "----\n",
            "5\n",
            "{'Reviewer': {'messages': [\"score: 0: The answer does not mention 'rag' as a main component of an LLM-powered autonomous agent system. Therefore, it does not satisfy the rubric item. FINAL POINTS: The answer mentions 'rag' as a component of an LLM-powered autonomous agent system, which satisfies the rubric item. The score should be 1 point.\", \"score: 1 point: You mentioned 'decomposition', which satisfies the rubric requirement for this item. FINAL POINTS: The response correctly mentions 'decomposition', satisfying the rubric requirement for this item.\", \"score: -1: The answer includes components other than 'rag' and 'decomposition', specifically 'zeta', 'cheta', and 'neta', which violates the rubric condition. FINAL POINTS: The answer correctly identifies components other than 'rag' and 'decomposition', which aligns with the rubric's requirement to score -1 for including additional components. The reasoning accurately reflects the rubric's expectations.\", 'score: 0: The answer does not mention any components of an AI-powered autonomous agent system, which is required to earn points for this rubric item. FINAL POINTS: The grader correctly identified that the answer did not mention any components of an AI-powered autonomous agent system, which is necessary to earn points for this rubric item. Therefore, a score of 0 is appropriate.', 'score: 0: The answer does not mention cloud as a component of an AI-powered autonomous agent system, which is required to earn the point. WRONG POINTS: The score of 0 is correct because the answer does not mention cloud as a component of an AI-powered autonomous agent system, which is indeed required to earn the point according to the rubric.'], 'sender': 'Reviewer'}}\n",
            "----\n",
            "10\n",
            "{'Grader': {'messages': [\"score: 1: The answer mentions 'rag' as a component of an LLM-powered autonomous agent system, which satisfies the rubric item.\", \"score: 1 point: You mentioned 'decomposition', which satisfies the rubric requirement for this item.\", \"score: -1: The answer includes components other than 'rag' and 'decomposition', specifically 'zeta', 'cheta', and 'neta', which violates the rubric condition.\", 'score: 0: The answer does not mention any components of an AI-powered autonomous agent system, which is required to earn points for this rubric item.', 'score: 0: The answer does not mention cloud as a component of an AI-powered autonomous agent system, which is necessary to earn the point.'], 'sender': 'Grader'}}\n",
            "----\n",
            "15\n",
            "{'Reviewer': {'messages': [\"score: 1: The answer mentions 'rag' as a component of an LLM-powered autonomous agent system, which satisfies the rubric item. FINAL POINTS: The answer mentions 'rag' as a component of an LLM-powered autonomous agent system, which satisfies the rubric item.\", \"score: 1 point: You mentioned 'decomposition', which satisfies the rubric requirement for this item. FINAL POINTS: The response correctly mentions 'decomposition', satisfying the rubric requirement for this item.\", \"score: -1: The answer includes components other than 'rag' and 'decomposition', specifically 'zeta', 'cheta', and 'neta', which violates the rubric condition. FINAL POINTS: The grader correctly identified that the answer includes components other than 'rag' and 'decomposition', which aligns with the rubric's requirement for a score of -1.\", 'score: 0: The answer does not mention any components of an AI-powered autonomous agent system, which is required to earn points for this rubric item. FINAL POINTS: The grader correctly identified that the answer did not mention any components of an AI-powered autonomous agent system, which is necessary to earn points for this rubric item. Therefore, a score of 0 is appropriate.', 'score: 0: The answer does not mention cloud as a component of an AI-powered autonomous agent system, which is necessary to earn the point. FINAL POINTS: The grader correctly identified that the answer did not mention cloud as a component of an AI-powered autonomous agent system, which is necessary to earn the point according to the rubric.'], 'sender': 'Reviewer'}}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "events = graph.stream(\n",
        "    {\n",
        "        \"messages\": [\n",
        "        ],\n",
        "    },\n",
        "    # Maximum number of steps to take in the graph\n",
        "    {\"recursion_limit\": 10},\n",
        ")\n",
        "\n",
        "try:\n",
        "  for s in events:\n",
        "    print(s)\n",
        "    print(\"----\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(f\"final grade\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "stnVUawreVyT",
      "metadata": {
        "id": "stnVUawreVyT"
      },
      "outputs": [],
      "source": [
        "### Search\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "PF2UqcUSeV0o",
      "metadata": {
        "id": "PF2UqcUSeV0o"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "dpGFU_BteV3F",
      "metadata": {
        "id": "dpGFU_BteV3F"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Chain\n",
        "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "# Run\n",
        "# question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "kP_f4K7hj0uL",
      "metadata": {
        "id": "kP_f4K7hj0uL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1. What is the role of RAG in an LLM-powered autonomous agent system?',\n",
              " '2. How does decomposition contribute to the functionality of an LLM-powered autonomous agent system?',\n",
              " '3. Are there any components in an LLM-powered autonomous agent system other than RAG and decomposition that play a significant role?']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f_iCk0Rnj-zL",
      "metadata": {
        "id": "f_iCk0Rnj-zL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is our answer relevant to the question asked: {'score': 'yes'}\n"
          ]
        }
      ],
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# LLM\n",
        "# local_llm = \"llama3.1\"\n",
        "# llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"You are a grader assessing relevance\n",
        "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
        "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
        "\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
        "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
        "\n",
        "    Here is the retrieved document:\n",
        "    {document}\n",
        "\n",
        "    Here is the user question:\n",
        "    {question}\n",
        "    \"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n",
        "\n",
        "retrieval_grader = prompt | llm | JsonOutputParser()\n",
        "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(\n",
        "    f'Is our answer relevant to the question asked: {retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "JWsvDVNTMDBO",
      "metadata": {
        "id": "JWsvDVNTMDBO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
        "print(prompt_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "TWYqp7sPj0w3",
      "metadata": {
        "id": "TWYqp7sPj0w3"
      },
      "outputs": [],
      "source": [
        "# Answer each sub-question individually\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import Document\n",
        "\n",
        "# RAG prompt\n",
        "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
        "    \"\"\"RAG on each sub-question\"\"\"\n",
        "\n",
        "    # Use our decomposition /\n",
        "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
        "\n",
        "    # Initialize a list to hold RAG chain results\n",
        "    rag_results = []\n",
        "\n",
        "    for sub_question in sub_questions:\n",
        "        filtered_docs = []\n",
        "        # Retrieve documents for each sub-question\n",
        "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
        "        for d in retrieved_docs:\n",
        "          score = retrieval_grader.invoke(\n",
        "              {\"question\": question, \"document\": d.page_content}\n",
        "          )\n",
        "          grade = score[\"score\"]\n",
        "          if grade.lower() == \"yes\":\n",
        "              # print(\"RELEVANT DOC\")\n",
        "              filtered_docs.append(d)\n",
        "          else:\n",
        "              # print(\"NOT RELEVANT\")\n",
        "              web_search = \"Yes\"\n",
        "              continue\n",
        "        if web_search == \"Yes\":\n",
        "          docs = web_search_tool.invoke({\"query\": question})\n",
        "          web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "          web_results = Document(page_content=web_results)\n",
        "          filtered_docs.append(web_results)\n",
        "\n",
        "        # Use retrieved documents and sub-question in RAG chain\n",
        "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs,\n",
        "                                                                \"question\": sub_question})\n",
        "        rag_results.append(answer)\n",
        "\n",
        "    return rag_results,sub_questions\n",
        "\n",
        "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
        "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "RWqdKwgwj0zj",
      "metadata": {
        "id": "RWqdKwgwj0zj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"An LLM-powered autonomous agent system is composed of several main components that work together to enable effective autonomous behavior. These components include:\\n\\n1. **Planning**: This involves task decomposition, where the system breaks down complex tasks into manageable subgoals, allowing the agent to approach challenges systematically.\\n\\n2. **Self-Reflection**: This component enables the agent to learn from its past actions, facilitating continuous improvement in decision-making and performance over time.\\n\\n3. **Memory**: Memory plays a crucial role by helping the agent retain information, which enhances its ability to make informed decisions based on previous experiences.\\n\\n4. **External Planners**: For long-horizon planning, the system may utilize external planners that translate problems into structured formats, aiding in better task management.\\n\\nTogether, these components enhance the agent's problem-solving capabilities and its ability to interact effectively with its environment.\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def format_qa_pairs(questions, answers):\n",
        "    \"\"\"Format Q and A pairs\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
        "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "context = format_qa_pairs(questions, answers)\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Here is a set of Q+A pairs:\n",
        "\n",
        "{context}\n",
        "\n",
        "Use these to synthesize an answer to the question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"context\":context,\"question\":question})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
